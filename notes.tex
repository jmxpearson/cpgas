\documentclass[11pt]{article}
\usepackage{amssymb,amsmath}

\begin{document}
\title{Changepoint gas}
\author{John Pearson}
\maketitle

\section{Problem}
We would like to perform rapid inference for discrete latent variables in long time series. For Hidden Markov Models, forward-backward inference is $\mathcal{O}(TM^2)$ with $M$ the number of latent states and $T$ the time. However, for hidden semi-Markov models, the run time is $\mathcal{O}(TM^2 D)$ with $D$ the maximum duration of a state.

However, in many cases where semi-Markov models would naturally be applied, transitions are sparse, and posteriors for the state variables approximately alternate between periods of high posterior certainty punctuated by uncertain transition regions of disparate length scales.

\section{Posterior Approximation}
For the case of variational inference over binary latent variables $z(t)$, we will switch from a representation in terms of states to a representation in terms of changepoints $c_k$. That is, for a given index $k$ representing a transition $0 \rightarrow 1$, we will have $z(t < c_k) = 0$ and $z(t \ge c_k) = 1$. Thus, $z(t)$ and $c_k$ are equivalent descriptions of the model, linked deterministically, and $\mathcal{H}[q(z)] = \mathcal{H}[q(c)]$, where $\mathcal{H}$ is the entropy of the distribution.

As a first approximation, then, we will use a mean field approximation for the changepoint locations
\begin{equation}
\label{qc}
    q(c) = \prod_{k = 1}^K \mathcal{N}(\mu_k, \sigma^2_k)
\end{equation}
for which we can easily calculate
\begin{equation}
    \mathcal{H}[q(c)] = \frac{K}{2}(\log 2\pi + 1) + \sum_k \log \sigma_k
\end{equation}
Clearly, entropy scales with the number of changepoints and the uncertainty of the changepoint locations. Note also that we have assumed a fixed number of changepoints, $K$, which, like $\mu$ and $\sigma$ should be thought of as a variational parameter to be optimized.\footnote{Of course, with variational inference, which maximizes the sum of the expected log evidence and the entropy, both terms should benefit from $K$ larger. Thus, constraining $K$ will rely on the prior over changepoints, which should also appropriately scale with $K$.}

\section{Example: Poisson firing}
Here, we consider the example of a Poisson process driven by such a binary latent state. That is, we observe in successive time bins of size $\Delta t$ event counts $N_t$ with instantaneous rate
\begin{equation}
    r(t) = \lambda \nu^{z(t)}
\end{equation}
Thus, if we write $t = k\Delta t$, we have
\begin{equation}
    N_k \sim \text{Pois}\left(\lambda \int_{(k - 1)\Delta t}^{k\Delta t} \nu^{z(t)} dt\right)
\end{equation}
Now, we would like to calculate the expectation of the log of this evidence under the variational posterior, for which we need
\begin{align}
    \mathbb{E}_q[\log p(N, z)] &= \mathbb{E}_q[\log p(N|z)] + \mathbb{E}_q[p(z)] \\
    &= \sum_k \left(N_k \mathbb{E}_q \left[ \log \int_{(k - 1)\Delta t}^{k\Delta t} \lambda \nu^{z(t)} dt \right] \right. \\
    &- \left.\lambda \mathbb{E}_q\left[ \int_{(k - 1)\Delta t}^{k\Delta t} \nu^{z(t)} dt\right]\right)
    + \mathbb{E}_q[p(z)]
\end{align}
Now assume for the moment we have the marginal distribution at each time: $\zeta(t) = \mathbb{E}_q[z(t)]$. Then
\begin{align}
    \sum_k \mathbb{E}_q\left[ \int_{(k - 1)\Delta t}^{k\Delta t} \nu^{z(t)} dt\right] &= \int \mathbb{E}_q \left[\nu^{z(t)}\right] dt = (1 - Z) + Z\nu
\end{align}
where
\begin{equation}
    Z \equiv \int \zeta(t) dt
\end{equation}
The expectation of the log integral is easy to compute if the change point is on either side of the interval in question. If we let $t_k \equiv k \Delta t$, then if the nearest changepoint $c_k \notin [t_{k -1}, t_k)$ the value of the integral is
\begin{equation}
    \int_{t_{k - 1}}^{t_k} \nu^{z(t)} = \nu^{z_k}(\Delta t)
\end{equation}
where $z_k$ is the value of the state in that bin. Thus
\begin{align}
    \sum_k M_k \mathbb{E}_q \left[
    \log \int_{t_{k - 1}}^{t_k} \nu^{z(t)}
    \right]
    &\approx \sum_k M_k \mathbb{E}_q \left[z_k \log \nu \right] \Delta t = \left[\int \rho(t) \zeta(t) dt \right] \log \nu
\end{align}
That is, we are averaging the expected value of $z$, $\zeta$, against the instantaneous event rate of the process
\begin{equation}
    \rho(t) = \sum_i \delta(t - t_i)
\end{equation}
with $i$ indexing events. Note that the approximation above is exact when the changepoints are located at the bin boundaries, since $z(t)$ is then constant within each bin.\footnote{I suspect, however, that the last expression is exact even without this assumption.} However, this requires us to use a discrete approximation to $p(c)$. We will return to this below.

With these results in hand, we can then calculate the variational objective
\begin{align}
    \mathcal{L} &= \mathbb{E}_q [\log p(M, z)] + \mathcal{H}[q(z)] \\
    &= M \log \lambda + \overline{M} \log \nu - \lambda [(T - Z) + Z\nu] + \mathbb{E}_q[p(c)] \\
    &+ \frac{K}{2}\log(2\pi e) + \sum_k \log \sigma_k
\end{align}
where we have defined
\begin{align}
    M &\equiv \int \rho(t) dt &
    \overline{M} &\equiv \int \rho(t) \zeta(t) dt
\end{align}
and replaced the prior on $z$ with the prior on $c$, since the two are equivalent. Finally, we need to use the ansatz (\ref{qc}) to calculate
\begin{equation}
    \zeta(t) = \mathbb{E}_q[z(t)] = \sum_k Q_k \Phi\left(\frac{t - \mu_k}{\sigma_k} \right)
\end{equation}
For the binary case, we have, without loss of generality, $Q_k = (-1)^k$, and we further assume that the changepoints are sparse enough that only the closest pair of changepoints to a given time contribute meaningfully to the sum. We can then easily calculate
\begin{align}
    \frac{\partial \zeta}{\partial \mu_k} &= -\frac{Q_k}{\sigma_k}\phi\left( \frac{t - \mu_k}{\sigma_k}\right) \\
    \frac{\partial \zeta}{\partial \sigma_k} &= -\frac{t - \mu_k}{\sigma_k^2} Q_k\phi\left( \frac{t - \mu_k}{\sigma_k}\right)
\end{align}
with $\phi(x)$ the standard normal density. As a result
\begin{align}
    \frac{\partial Z}{\partial \mu_k} &= \int \frac{\partial \zeta}{\partial \mu_k} dt = -Q_k \\
    \frac{\partial Z}{\partial \sigma_k} &= \int \frac{\partial \zeta}{\partial \sigma_k} dt = 0
\end{align}
These results are intuitive: $Z$ increases as we move the changepoint to the left (for a $0 \rightarrow 1$ transition with $Q = 1$), and the width of the transition region is immaterial when integrated over all time. For the observations, we have
\begin{align}
    \frac{\partial \overline{M}}{\partial \mu_k} &= \int \rho(t) \frac{\partial \zeta}{\partial \mu_k} dt = -\frac{Q_k}{\sigma_k}\sum_i \phi\left( \frac{t_i - \mu_k}{\sigma_k}\right) \\
    \frac{\partial \overline{M}}{\partial \sigma_k} &= \int \rho(t) \frac{\partial \zeta}{\partial \sigma_k} dt = Q_k\sum_i \frac{t_i - \mu_k}{\sigma_k}\phi\left( \frac{t_i - \mu_k}{\sigma_k}\right)
\end{align}
Therefore, in the case that the prior on changepoints, $p(c)$ is flat, we have the gradients
\begin{align}
    \frac{\partial \mathcal{L}}{\partial \mu_k} &= -\frac{Q_k}{\sigma_k}\sum_i \phi\left( \frac{t_i - \mu_k}{\sigma_k}\right) \log (\lambda \nu)
    + \lambda (\nu - 1) Q_k \\
    \frac{\partial \mathcal{L}}{\partial \sigma_k} &=
    Q_k\sum_i \frac{t_i - \mu_k}{\sigma_k}\phi\left( \frac{t_i - \mu_k}{\sigma_k}\right) \log \nu + \frac{1}{\sigma_k}
\end{align}

\section{MAP approach}
As a first pass, we can also consider a maximum \emph{a posteriori} approach in which we only look for the (highest posterior probability) locations of changepoints. In this case, we simply replace $z(t)$ with either 0 or 1, giving
\begin{align}
    \mathcal{L} &= M \log \lambda + M_1 \log \nu - \lambda T [1 - f_1 + f_1 \nu] + \mathbb{E}_q[p(c)] + \frac{K}{2} \log (2\pi e\sigma^2_{\mathrm{min}})
\end{align}
with $M$ the total number of events $M_1$ number of events that happen in state 1, and $f_1 = T_1 / T$ the fraction of total time $T$ spent in state 1. Here, we have not entirely eliminated the uncertainty $\sigma$, but have replaced it with $\sigma_{\mathrm{min}}$, a minimum width that is perhaps smaller than the measurement time resolution, so that $\log \sigma_{\mathrm{min}}$ can be treated as a large negative constant. As a result, when attempting to maximize $\mathcal{L}$, there is a penalty for increasing $K$, in addition to whatever prior penalty is imposed by $p(c)$.

Alternately, we can rewrite this maximization objective as minimization objective equal to a sum over cost functions for each partition plus a penalty on large partition numbers:
\begin{align}
    \mathcal{L}' &= \mathcal{C}(\ell) + K \Delta \\
    \mathcal{C}(\ell) &=
    \begin{cases}
        -M_\ell \log \lambda + \lambda \ell - p(\ell|0) & z_\ell = 0 \\
        -M_\ell \log \lambda\nu + \lambda\nu \ell - p(\ell|1) & z_\ell = 1
    \end{cases}
\end{align}
where $\ell$ denotes the partition, $z_\ell$ the partition label, $M_\ell$ the number of events inside the partition, and $p(\ell|z_\ell)$ the prior over partition sizes. Furthermore, we have assumed a fixed penalty for each changepoint, $\Delta = \log \sqrt{2\pi \sigma^2_\mathrm{min}}$.

Unfortunately, this cost function does not (naively) allow for optimizations such as PELT (Killick et al., 2012), which require that adding a changepoint always decrease the cost, so that effective pruning becomes possible. In fact, there are two key differences:
\begin{itemize}
    \item Because we have only two possible states (the same would be true for any fixed, finite number), changepoints must enter in pairs to preserve boundary conditions. That is, when looking at a region with $z = 1$, the correct operation to attempt is inserting \emph{two} changepoints inside it, i.e., inserting a region of $z = 0$. Boundary conditions are thus more complicated, and search has to range over two endpoints, not one.

    \item It is usually safe to assume that the prior over state durations, $p(\ell|z)$, is unimodal, but it is certainly not monotonic in $\ell$, which adds additional complexity to the optimization problem.
\end{itemize}
\end{document}